{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging using LSTMs in Keras\n",
    "\n",
    "This notebook trains a bidirectional LSTM network to tag parts-of-speech (POS), e.g. nouns, adjectives, adverbs in sentences. In other words: the network should learn to identify nouns, adjectives, etc. in sentences.\n",
    "\n",
    "This was more an exercise on how to use LSTM networks on sequences in Keras. I haven't checked if a simple hash table would actually perform better on this task :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the POS tags from the CoNLL-2000 dataset for training/testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    \n",
    "    sentences = []\n",
    "    sentences_labels = []\n",
    "    \n",
    "    sentence = []\n",
    "    sentence_labels = []\n",
    "    \n",
    "    with gzip.open(filename, 'rt') as f:\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                sentences.append(sentence)\n",
    "                sentences_labels.append(sentence_labels)\n",
    "                sentence = []\n",
    "                sentence_labels = []\n",
    "            else:\n",
    "                word, label, _ = line.split(' ')\n",
    "                sentence.append(word)\n",
    "                sentence_labels.append(label)\n",
    "                \n",
    "    return sentences, sentences_labels\n",
    "                \n",
    "train_words, train_labels = load_data('data/train.txt.gz')\n",
    "test_words, test_labels = load_data('data/test.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentence from the training set, including labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=1><tr><td>Meanwhile</td><td>RB</td></tr><tr><td>,</td><td>,</td></tr><tr><td>surviving</td><td>VBG</td></tr><tr><td>Texas</td><td>NNP</td></tr><tr><td>banking</td><td>NN</td></tr><tr><td>institutions</td><td>NNS</td></tr><tr><td>are</td><td>VBP</td></tr><tr><td>busily</td><td>RB</td></tr><tr><td>pitching</td><td>VBG</td></tr><tr><td>themselves</td><td>PRP</td></tr><tr><td>as</td><td>IN</td></tr><tr><td>the</td><td>DT</td></tr><tr><td>only</td><td>JJ</td></tr><tr><td>lenders</td><td>NNS</td></tr><tr><td>who</td><td>WP</td></tr><tr><td>truly</td><td>RB</td></tr><tr><td>care</td><td>VBP</td></tr><tr><td>about</td><td>IN</td></tr><tr><td>the</td><td>DT</td></tr><tr><td>state</td><td>NN</td></tr><tr><td>.</td><td>.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = len(train_words)\n",
    "\n",
    "i = random.randint(0, N-1)\n",
    "table = '<table border=1>'\n",
    "for word, label in zip(train_words[i], train_labels[i]):\n",
    "    table += '<tr><td>{}</td><td>{}</td></tr>'.format(word,label)\n",
    "table += '</table>'\n",
    "display(HTML(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a simple encoding for words and labels: integers. Unknown words are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_encoding(items):\n",
    "    items_by_freq = [item for item, _ in collections.Counter(items).most_common()]\n",
    "    item2enc = collections.defaultdict(lambda: 0, {enc: item for item, enc in enumerate(items_by_freq, 1)})\n",
    "    return item2enc\n",
    "    \n",
    "all_words = [word for words in train_words for word in words]\n",
    "word2enc = get_encoding(all_words)\n",
    "enc2word = collections.defaultdict(lambda: \"N/A\", {enc: item for item, enc in word2enc.items()})\n",
    "\n",
    "all_labels = [label for labels in train_labels for label in labels]\n",
    "label2enc = get_encoding(all_labels)\n",
    "enc2label = collections.defaultdict(lambda: \"N/A\", {enc: item for item, enc in label2enc.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_enc = [[word2enc[word] for word in sentence] for sentence in train_words]\n",
    "train_labels_enc = [[label2enc[label] for label in labels] for labels in train_labels]\n",
    "\n",
    "test_words_enc = [[word2enc[word] for word in sentence] for sentence in test_words]\n",
    "test_labels_enc = [[label2enc[label] for label in sentence_labels] for sentence_labels in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad all sequences to maximum length and convert labels to a one-hot encoding for the last network layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Maximum sentence length: 78"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maxlen = max(len(sentence) for sentence in train_words)\n",
    "display(HTML('Maximum sentence length: {}'.format(maxlen)))\n",
    "\n",
    "n_classes = len(label2enc) + 1  # +1 for 0\n",
    "\n",
    "train_words_enc_padded = pad_sequences(train_words_enc, maxlen=maxlen)\n",
    "train_labels_enc_padded = pad_sequences(train_labels_enc, maxlen=maxlen)\n",
    "train_labels_enc_padded_onehot = np.eye(n_classes)[train_labels_enc_padded]\n",
    "\n",
    "test_words_enc_padded = pad_sequences(test_words_enc, maxlen=maxlen)\n",
    "test_labels_enc_padded = pad_sequences(test_labels_enc, maxlen=maxlen)\n",
    "test_labels_enc_padded_onehot = np.eye(n_classes)[test_labels_enc_padded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Keras-provided word embedding (conversion of the encoded word into a vector of 128 dimensions), \n",
    "a bidirectional LSTM layer and a dropout layer as hidden layers. The output layer is a dense softmax layer.\n",
    "\n",
    "The dense layer is \"time-distributed\" because we're dealing with sequences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2763520   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 45)          5805      \n",
      "=================================================================\n",
      "Total params: 2,868,141\n",
      "Trainable params: 2,868,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8936 samples, validate on 2012 samples\n",
      "Epoch 1/5\n",
      "8936/8936 [==============================] - 54s - loss: 1.7767 - acc: 0.5530 - val_loss: 0.3213 - val_acc: 0.9324\n",
      "Epoch 2/5\n",
      "8936/8936 [==============================] - 53s - loss: 0.2340 - acc: 0.9476 - val_loss: 0.0928 - val_acc: 0.9772\n",
      "Epoch 3/5\n",
      "8936/8936 [==============================] - 53s - loss: 0.0883 - acc: 0.9794 - val_loss: 0.0606 - val_acc: 0.9830\n",
      "Epoch 4/5\n",
      "8936/8936 [==============================] - 53s - loss: 0.0520 - acc: 0.9872 - val_loss: 0.0509 - val_acc: 0.9853\n",
      "Epoch 5/5\n",
      "8936/8936 [==============================] - 53s - loss: 0.0368 - acc: 0.9905 - val_loss: 0.0478 - val_acc: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1cb0b52ac8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(word2enc)+1, 128, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_words_enc_padded, \n",
    "          train_labels_enc_padded_onehot,\n",
    "          validation_data=(test_words_enc_padded,\n",
    "                           test_labels_enc_padded_onehot),\n",
    "          batch_size=32, \n",
    "\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some example predictions for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>word</th><th>pred</th><th>label</th></tr><tr><td>The</td><td style=\"color: green\">DT</td><td>DT</td></tr><tr><td>Nasdaq</td><td style=\"color: green\">NNP</td><td>NNP</td></tr><tr><td>100</td><td style=\"color: green\">CD</td><td>CD</td></tr><tr><td>Index</td><td style=\"color: green\">NNP</td><td>NNP</td></tr><tr><td>began</td><td style=\"color: green\">VBD</td><td>VBD</td></tr><tr><td>the</td><td style=\"color: green\">DT</td><td>DT</td></tr><tr><td>day</td><td style=\"color: green\">NN</td><td>NN</td></tr><tr><td>at</td><td style=\"color: green\">IN</td><td>IN</td></tr><tr><td><i>N/A</i></td><td style=\"color: red\">IN</td><td>CD</td></tr><tr><td>,</td><td style=\"color: green\">,</td><td>,</td></tr><tr><td>lost</td><td style=\"color: green\">VBD</td><td>VBD</td></tr><tr><td>2</td><td style=\"color: green\">CD</td><td>CD</td></tr><tr><td>%</td><td style=\"color: green\">NN</td><td>NN</td></tr><tr><td>at</td><td style=\"color: green\">IN</td><td>IN</td></tr><tr><td>one</td><td style=\"color: green\">CD</td><td>CD</td></tr><tr><td>point</td><td style=\"color: green\">NN</td><td>NN</td></tr><tr><td>,</td><td style=\"color: green\">,</td><td>,</td></tr><tr><td>and</td><td style=\"color: green\">CC</td><td>CC</td></tr><tr><td>was</td><td style=\"color: green\">VBD</td><td>VBD</td></tr><tr><td>up</td><td style=\"color: green\">IN</td><td>IN</td></tr><tr><td>0.4</td><td style=\"color: green\">CD</td><td>CD</td></tr><tr><td>%</td><td style=\"color: green\">NN</td><td>NN</td></tr><tr><td>at</td><td style=\"color: green\">IN</td><td>IN</td></tr><tr><td>another</td><td style=\"color: green\">DT</td><td>DT</td></tr><tr><td>.</td><td style=\"color: green\">.</td><td>.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>word</th><th>pred</th><th>label</th></tr><tr><td>Brown</td><td style=\"color: green\">NNP</td><td>NNP</td></tr><tr><td>'s</td><td style=\"color: green\">POS</td><td>POS</td></tr><tr><td>story</td><td style=\"color: green\">NN</td><td>NN</td></tr><tr><td>:</td><td style=\"color: green\">:</td><td>:</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = np.random.randint(0, len(test_words), size=2)\n",
    "\n",
    "samples = test_words_enc_padded[i]\n",
    "predictions = model.predict_classes(samples, verbose=False)\n",
    "labelss = test_labels_enc_padded[i]\n",
    "\n",
    "for sample, prediction, labels in zip(samples, predictions, labelss):\n",
    "   \n",
    "    in_padding = True\n",
    "    \n",
    "    table = '<table>'\n",
    "    table += '<tr><th>word</th><th>pred</th><th>label</th></tr>'\n",
    "    for word, predicted_label, label in zip(sample,  prediction, labels):\n",
    "        if in_padding and word == 0:\n",
    "            continue\n",
    "        else:\n",
    "            in_padding = False\n",
    "            \n",
    "        table += '<tr><td>{}</td><td style=\"color: {}\">{}</td><td>{}</td></tr>'.format(\n",
    "            enc2word[word] if word != 0 else \"<i>N/A</i>\",\n",
    "            \"green\" if predicted_label == label else \"red\",\n",
    "            enc2label[predicted_label],\n",
    "            enc2label[label]\n",
    "        )\n",
    "    table += '</table>'\n",
    "    display(HTML(table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
